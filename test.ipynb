{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from utils.TSDataset import TimeSeriesDataset\n",
    "from utils.TSDataset import data_load\n",
    "from utils.plot import *\n",
    "from utils.split_train_val_test import *\n",
    "from utils.compute_metric import compute_metrics, append_score\n",
    "from utils.compute_metric import compute_metrics_seq2seq\n",
    "from utils.models import cnn_lstm_model_eval\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "project_dir\n",
    "\n",
    "data_path = os.path.join(project_dir, 'data', 'processed', 'BTC-USD-sample.csv')\n",
    "\n",
    "# Load data\n",
    "data1, x_scaler1, y_scaler1 = data_load(data_path, x_scaler='minmax', y_scaler='minmax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Step Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "record = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'Naive', 'h-step Forecast': 1, 'mse': 1160.3737790527032, 'mae': 25.10380253502343, 'huber': np.float32(24.65289)}\n",
      "{'model': 'Naive', 'h-step Forecast': 10, 'mse': 5275.25768559061, 'mae': 54.060173825503384, 'huber': np.float32(53.572163)}\n",
      "{'model': 'Naive', 'h-step Forecast': 60, 'mse': 26082.668655180554, 'mae': 116.48661504629631, 'huber': np.float32(115.99019)}\n"
     ]
    }
   ],
   "source": [
    "forecast_horizons = [1, 10, 60]\n",
    "train_df, test_df = split_train_val_test(data1, train_frac=0.7)\n",
    "scores = []\n",
    "target_col = 'y'\n",
    "\n",
    "test_target = test_df[target_col].values\n",
    "\n",
    "for h in forecast_horizons:\n",
    "    naive_y_true = []  # will hold the true future values for every forecasting window\n",
    "    naive_y_pred = []  # will hold the corresponding naive predictions\n",
    "    \n",
    "    for i in range(len(test_target) - h):\n",
    "        # True values: for instance, at time step 0, this gets indices 1 to h (i.e., 1:11 when h=10)\n",
    "        y_true = test_target[i + 1: i + h + 1]\n",
    "        # Naive predictions: create an array of length h filled with the value at time step i\n",
    "        y_pred = np.full((h,), test_target[i])\n",
    "        \n",
    "        naive_y_true.append(y_true)\n",
    "        naive_y_pred.append(y_pred)\n",
    "    \n",
    "    naive_y_true = y_scaler1.inverse_transform(naive_y_true)\n",
    "    naive_y_pred = y_scaler1.inverse_transform(naive_y_pred)\n",
    "    mse_naive, mae_naive, huber_naive = compute_metrics(naive_y_true, naive_y_pred)\n",
    "    \n",
    "    # Print out the performance for this forecast horizon\n",
    "    record = {\n",
    "        'model': 'Naive',\n",
    "        'h-step Forecast': h,\n",
    "        'mse': mse_naive,\n",
    "        'mae': mae_naive,\n",
    "        'huber': huber_naive\n",
    "    }\n",
    "    append_score(scores, record)\n",
    "for i in scores:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 100\n",
    "target_col = 'y'\n",
    "feature_cols = ['x1', 'x2', 'x3', 'x4', 'x5']\n",
    "return_index = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "MSE for 1-step forecast: 339.7964571708816\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "MSE for 10-step forecast: 8107.401599442298\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "MSE for 60-step forecast: 50929.79013949266\n"
     ]
    }
   ],
   "source": [
    "data_frame = data1\n",
    "feature_cols = ['x1', 'x2', 'x3', 'x4', 'x5']\n",
    "target_col = 'y'\n",
    "return_index = True\n",
    "scaler = y_scaler1\n",
    "filters = 26\n",
    "window_size = 129\n",
    "kernel_size = 2\n",
    "strides = 1\n",
    "lstm_units = 31\n",
    "learning_rate = 0.01835\n",
    "epochs = 87\n",
    "forecast_horizon = [1, 10, 60]\n",
    "seq2seq = True\n",
    "\n",
    "for h in forecast_horizon:\n",
    "    mse = -cnn_lstm_model_eval(data_frame = data_frame, \n",
    "                        feature_cols = feature_cols, \n",
    "                        target_col = target_col, \n",
    "                        return_index = return_index, \n",
    "                        scaler = scaler, \n",
    "                        filters = filters, \n",
    "                        window_size = window_size, \n",
    "                        kernel_size = kernel_size, \n",
    "                        strides = strides, \n",
    "                        lstm_units = lstm_units, \n",
    "                        learning_rate = learning_rate, \n",
    "                        epochs = epochs, \n",
    "                        seq2seq = seq2seq, \n",
    "                        forecast_horizon = h)\n",
    "    print(f\"MSE for {h}-step forecast: {mse}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 500us/step\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 514us/step\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 518us/step\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "for h in forecast_horizons:\n",
    "    train_df, test_df = split_train_val_test(data1, train_frac=0.7)\n",
    "    train_set = TimeSeriesDataset(dataframe=train_df, window_size=window_size, forecast_horizon=h, feature_cols=feature_cols, target_col=target_col, return_index=return_index)\n",
    "    test_set = TimeSeriesDataset(dataframe=test_df, window_size=window_size, forecast_horizon=h, feature_cols=feature_cols, target_col=target_col, return_index=return_index)\n",
    "\n",
    "    X_train, y_train, x_dates, y_dates = train_set.X_seq, train_set.y_seq, train_set.x_dates, train_set.y_dates\n",
    "    X_test, y_test, x_dates_test, y_dates_test = test_set.X_seq, test_set.y_seq, test_set.x_dates, test_set.y_dates\n",
    "\n",
    "    dense_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(window_size, len(feature_cols))),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(h)\n",
    "    ])\n",
    "\n",
    "    dense_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "    history_dense = dense_model.fit(X_train, y_train, epochs=50,\n",
    "                                    validation_data=(X_test, y_test), verbose=0)\n",
    "\n",
    "    #plot_learning_curves(history_dense.history)\n",
    "    y_pred = dense_model.predict(X_test)\n",
    "    y_pred = y_scaler1.inverse_transform(y_pred.reshape(-1, 1))\n",
    "    y_true = y_scaler1.inverse_transform(y_test.reshape(-1, 1))\n",
    "    \n",
    "    mse_dense, mae_dense, huber_dense = compute_metrics(y_true, y_pred)\n",
    "    \n",
    "    record = {\n",
    "        'model': 'Dense',\n",
    "        'h-step Forecast': h,\n",
    "        'mse': mse_dense,\n",
    "        'mae': mae_dense,\n",
    "        'huber': huber_dense\n",
    "    }\n",
    "    append_score(scores, record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeddyxie/CNN-LSTM/CNN-LSTM/venv/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeddyxie/CNN-LSTM/CNN-LSTM/venv/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeddyxie/CNN-LSTM/CNN-LSTM/venv/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeddyxie/CNN-LSTM/CNN-LSTM/venv/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "for h in forecast_horizons:\n",
    "    train_df, test_df = split_train_val_test(data1, train_frac=0.7)\n",
    "    train_set = TimeSeriesDataset(dataframe=train_df, window_size=window_size, forecast_horizon=h, feature_cols=feature_cols, target_col=target_col, return_index=return_index)\n",
    "    test_set = TimeSeriesDataset(dataframe=test_df, window_size=window_size, forecast_horizon=h, feature_cols=feature_cols, target_col=target_col, return_index=return_index)\n",
    "\n",
    "    X_train, y_train, x_dates, y_dates = train_set.X_seq, train_set.y_seq, train_set.x_dates, train_set.y_dates\n",
    "    X_test, y_test, x_dates_test, y_dates_test = test_set.X_seq, test_set.y_seq, test_set.x_dates, test_set.y_dates\n",
    "\n",
    "    rnn_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(window_size, len(feature_cols))),\n",
    "        tf.keras.layers.SimpleRNN(units=20),\n",
    "        tf.keras.layers.Dense(h)\n",
    "    ])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    rnn_model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "    history_rnn = rnn_model.fit(X_train, y_train, epochs=20,\n",
    "                                validation_data=(X_test, y_test), verbose=0)\n",
    "\n",
    "    y_pred = rnn_model.predict(X_test)\n",
    "    y_pred = y_scaler1.inverse_transform(y_pred.reshape(-1, 1))\n",
    "    y_true = y_scaler1.inverse_transform(y_test.reshape(-1, 1))\n",
    "    \n",
    "    mse_rnn, mae_rnn, huber_rnn = compute_metrics(y_true, y_pred)\n",
    "    \n",
    "    record = {\n",
    "        'model': 'Simple RNN',\n",
    "        'h-step Forecast': h,\n",
    "        'mse': mse_rnn,\n",
    "        'mae': mae_rnn,\n",
    "        'huber': huber_rnn\n",
    "    }\n",
    "    scores.append(record)\n",
    "    #plot_learning_curves(history_rnn.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeddyxie/CNN-LSTM/CNN-LSTM/venv/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeddyxie/CNN-LSTM/CNN-LSTM/venv/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeddyxie/CNN-LSTM/CNN-LSTM/venv/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeddyxie/CNN-LSTM/CNN-LSTM/venv/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "for h in forecast_horizons:\n",
    "    train_df, test_df = split_train_val_test(data1, train_frac=0.7)\n",
    "    train_set = TimeSeriesDataset(dataframe=train_df, window_size=window_size, forecast_horizon=h, feature_cols=feature_cols, target_col=target_col, return_index=return_index)\n",
    "    test_set = TimeSeriesDataset(dataframe=test_df, window_size=window_size, forecast_horizon=h, feature_cols=feature_cols, target_col=target_col, return_index=return_index)\n",
    "\n",
    "    X_train, y_train, x_dates, y_dates = train_set.X_seq, train_set.y_seq, train_set.x_dates, train_set.y_dates\n",
    "    X_test, y_test, x_dates_test, y_dates_test = test_set.X_seq, test_set.y_seq, test_set.x_dates, test_set.y_dates\n",
    "\n",
    "    rnn_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(window_size, len(feature_cols))),\n",
    "    tf.keras.layers.SimpleRNN(units=20, return_sequences=True),\n",
    "    tf.keras.layers.SimpleRNN(units=20, return_sequences=False),\n",
    "    tf.keras.layers.Dense(h)\n",
    "    ])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    rnn_model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "    history_rnn = rnn_model.fit(X_train, y_train, epochs=20,\n",
    "                                validation_data=(X_test, y_test), verbose=0)\n",
    "\n",
    "    y_pred = rnn_model.predict(X_test)\n",
    "    y_pred = y_scaler1.inverse_transform(y_pred.reshape(-1, 1))\n",
    "    y_true = y_scaler1.inverse_transform(y_test.reshape(-1, 1))\n",
    "    \n",
    "    mse_rnn, mae_rnn, huber_rnn = compute_metrics(y_true, y_pred)\n",
    "    \n",
    "    record = {\n",
    "        'model': 'Deep RNN',\n",
    "        'h-step Forecast': h,\n",
    "        'mse': mse_rnn,\n",
    "        'mae': mae_rnn,\n",
    "        'huber': huber_rnn\n",
    "    }\n",
    "    append_score(scores, record)\n",
    "    #plot_learning_curves(history_rnn.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "for h in forecast_horizons:\n",
    "    train_df, test_df = split_train_val_test(data1, train_frac=0.7)\n",
    "    train_set = TimeSeriesDataset(dataframe=train_df, window_size=window_size, forecast_horizon=h, feature_cols=feature_cols, target_col=target_col, return_index=return_index)\n",
    "    test_set = TimeSeriesDataset(dataframe=test_df, window_size=window_size, forecast_horizon=h, feature_cols=feature_cols, target_col=target_col, return_index=return_index)\n",
    "\n",
    "    X_train, y_train, x_dates, y_dates = train_set.X_seq, train_set.y_seq, train_set.x_dates, train_set.y_dates\n",
    "    X_test, y_test, x_dates_test, y_dates_test = test_set.X_seq, test_set.y_seq, test_set.x_dates, test_set.y_dates\n",
    "\n",
    "    rnn_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(window_size, len(feature_cols))),\n",
    "    tf.keras.layers.SimpleRNN(units=20, return_sequences=True),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.SimpleRNN(units=20, return_sequences=False),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(h)\n",
    "    ])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    rnn_model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "    history_rnn = rnn_model.fit(X_train, y_train, epochs=20,\n",
    "                                validation_data=(X_test, y_test), verbose=0)\n",
    "\n",
    "    y_pred = rnn_model.predict(X_test)\n",
    "    y_pred = y_scaler1.inverse_transform(y_pred.reshape(-1, 1))\n",
    "    y_true = y_scaler1.inverse_transform(y_test.reshape(-1, 1))\n",
    "    \n",
    "    mse_rnn, mae_rnn, huber_rnn = compute_metrics(y_true, y_pred)\n",
    "    \n",
    "    record = {\n",
    "        'model': 'RNN Batch Normalization',\n",
    "        'h-step Forecast': h,\n",
    "        'mse': mse_rnn,\n",
    "        'mae': mae_rnn,\n",
    "        'huber': huber_rnn\n",
    "    }\n",
    "    append_score(scores, record)\n",
    "    #plot_learning_curves(history_rnn.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "for h in forecast_horizons:\n",
    "    train_df, test_df = split_train_val_test(data1, train_frac=0.7)\n",
    "    train_set = TimeSeriesDataset(dataframe=train_df, window_size=window_size, forecast_horizon=h, feature_cols=feature_cols, target_col=target_col, return_index=return_index)\n",
    "    test_set = TimeSeriesDataset(dataframe=test_df, window_size=window_size, forecast_horizon=h, feature_cols=feature_cols, target_col=target_col, return_index=return_index)\n",
    "\n",
    "    X_train, y_train, x_dates, y_dates = train_set.X_seq, train_set.y_seq, train_set.x_dates, train_set.y_dates\n",
    "    X_test, y_test, x_dates_test, y_dates_test = test_set.X_seq, test_set.y_seq, test_set.x_dates, test_set.y_dates\n",
    "\n",
    "    rnn_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(window_size, len(feature_cols))),\n",
    "    tf.keras.layers.LSTM(units=20, return_sequences=True),\n",
    "    tf.keras.layers.LSTM(units=20, return_sequences=False),\n",
    "    tf.keras.layers.Dense(h)\n",
    "    ])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    rnn_model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "    history_rnn = rnn_model.fit(X_train, y_train, epochs=20,\n",
    "                                validation_data=(X_test, y_test), verbose=0)\n",
    "\n",
    "    y_pred = rnn_model.predict(X_test)\n",
    "    y_pred = y_scaler1.inverse_transform(y_pred.reshape(-1, 1))\n",
    "    y_true = y_scaler1.inverse_transform(y_test.reshape(-1, 1))\n",
    "    \n",
    "    mse_rnn, mae_rnn, huber_rnn = compute_metrics(y_true, y_pred)\n",
    "    \n",
    "    record = {\n",
    "        'model': 'LSTM',\n",
    "        'h-step Forecast': h,\n",
    "        'mse': mse_rnn,\n",
    "        'mae': mae_rnn,\n",
    "        'huber': huber_rnn\n",
    "    }\n",
    "    append_score(scores, record)\n",
    "    #plot_learning_curves(history_rnn.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "8817.181446333696\n",
      "73.64343211321192\n",
      "73.14473\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "forecast_horizons = [10]\n",
    "for h in forecast_horizons:\n",
    "    train_df, test_df = split_train_val_test(data1, train_frac=0.7)\n",
    "    train_set = TimeSeriesDataset(dataframe=train_df, window_size=window_size, forecast_horizon=h, feature_cols=feature_cols, target_col=target_col, return_index=return_index)\n",
    "    test_set = TimeSeriesDataset(dataframe=test_df, window_size=window_size, forecast_horizon=h, feature_cols=feature_cols, target_col=target_col, return_index=return_index)\n",
    "\n",
    "    X_train, y_train, x_dates, y_dates = train_set.X_seq, train_set.y_seq, train_set.x_dates, train_set.y_dates\n",
    "    X_test, y_test, x_dates_test, y_dates_test = test_set.X_seq, test_set.y_seq, test_set.x_dates, test_set.y_dates\n",
    "\n",
    "    rnn_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(window_size, len(feature_cols))),\n",
    "    tf.keras.layers.GRU(units=20, return_sequences=True),\n",
    "    tf.keras.layers.GRU(units=20, return_sequences=False),\n",
    "    tf.keras.layers.Dense(h)\n",
    "    ])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    rnn_model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "    history_rnn = rnn_model.fit(X_train, y_train, epochs=3,\n",
    "                                validation_data=(X_test, y_test), verbose=0)\n",
    "\n",
    "    y_pred = rnn_model.predict(X_test)\n",
    "\n",
    "    y_pred = y_scaler1.inverse_transform(y_pred.reshape(-1, 1))\n",
    "    y_true = y_scaler1.inverse_transform(y_test.reshape(-1, 1))\n",
    "    \n",
    "    mse_rnn, mae_rnn, huber_rnn = compute_metrics(y_true, y_pred)\n",
    "    print(mse_rnn)\n",
    "    print(mae_rnn)\n",
    "    print(huber_rnn)\n",
    "    \n",
    "    record = {\n",
    "        'model': 'GRU',\n",
    "        'h-step Forecast': h,\n",
    "        'mse': mse_rnn,\n",
    "        'mae': mae_rnn,\n",
    "        'huber': huber_rnn\n",
    "    }\n",
    "    append_score(scores, record)\n",
    "    #plot_learning_curves(history_rnn.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "window_size = 120\n",
    "for h in forecast_horizons:\n",
    "    train_df, test_df = split_train_val_test(data1, train_frac=0.7)\n",
    "    train_set = TimeSeriesDataset(dataframe=train_df, window_size=window_size, forecast_horizon=h, feature_cols=feature_cols, target_col=target_col, return_index=return_index)\n",
    "    test_set = TimeSeriesDataset(dataframe=test_df, window_size=window_size, forecast_horizon=h, feature_cols=feature_cols, target_col=target_col, return_index=return_index)\n",
    "\n",
    "    X_train, y_train, x_dates, y_dates = train_set.X_seq, train_set.y_seq, train_set.x_dates, train_set.y_dates\n",
    "    X_test, y_test, x_dates_test, y_dates_test = test_set.X_seq, test_set.y_seq, test_set.x_dates, test_set.y_dates\n",
    "\n",
    "   \n",
    "    cnn_model = keras.models.Sequential([\n",
    "    keras.layers.Input(shape=(window_size, len(feature_cols))),\n",
    "    keras.layers.Conv1D(filters=16, kernel_size=2, padding=\"valid\"),\n",
    "    keras.layers.LSTM(32, return_sequences=True),\n",
    "    keras.layers.LSTM(32, return_sequences=False),\n",
    "    keras.layers.Dense(h)\n",
    "    ])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.004717880792838919)\n",
    "    cnn_model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "    history_rnn = cnn_model.fit(X_train, y_train, epochs=20,\n",
    "                                validation_data=(X_test, y_test), verbose=0)\n",
    "\n",
    "    y_pred = cnn_model.predict(X_test)\n",
    "    y_pred = y_scaler1.inverse_transform(y_pred.reshape(-1, 1))\n",
    "    y_true = y_scaler1.inverse_transform(y_test.reshape(-1, 1))\n",
    "    \n",
    "    mse_rnn, mae_rnn, huber_rnn = compute_metrics(y_true, y_pred)\n",
    "    \n",
    "    cnn_lstm_record = {\n",
    "        'model': 'CNN-LSTM',\n",
    "        'h-step Forecast': h,\n",
    "        'mse': mse_rnn,\n",
    "        'mae': mae_rnn,\n",
    "        'huber': huber_rnn\n",
    "    }\n",
    "    append_score(scores, cnn_lstm_record)\n",
    "    #plot_learning_curves(history_rnn.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'Naive', 'h-step Forecast': 120, 'mse': 47600.81892224578, 'mae': 160.01141008454107, 'huber': np.float32(159.51382)}\n",
      "{'model': 'Naive', 'h-step Forecast': 240, 'mse': 92012.92070534195, 'mae': 222.9653111772487, 'huber': np.float32(222.46693)}\n",
      "{'model': 'Naive', 'h-step Forecast': 360, 'mse': 146950.88944180385, 'mae': 293.7249538255361, 'huber': np.float32(293.2261)}\n",
      "{'model': 'CNN-LSTM', 'h-step Forecast': 120, 'mse': 47717.291397079745, 'mae': 156.00167939646272, 'huber': np.float32(155.50249)}\n",
      "{'model': 'CNN-LSTM', 'h-step Forecast': 240, 'mse': 106428.28298186195, 'mae': 234.52212818255597, 'huber': np.float32(234.02264)}\n",
      "{'model': 'CNN-LSTM', 'h-step Forecast': 360, 'mse': 254819.54339690704, 'mae': 388.6029576857995, 'huber': np.float32(388.10324)}\n"
     ]
    }
   ],
   "source": [
    "for i in scores:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "window_size = 100\n",
    "forecast_horizons = 1\n",
    "\n",
    "train_df, test_df = split_train_val_test(data1, train_frac=0.7)\n",
    "train_set = TimeSeriesDataset(dataframe=train_df, window_size=window_size, forecast_horizon=h, feature_cols=feature_cols, target_col=target_col, return_index=return_index)\n",
    "test_set = TimeSeriesDataset(dataframe=test_df, window_size=window_size, forecast_horizon=h, feature_cols=feature_cols, target_col=target_col, return_index=return_index)\n",
    "\n",
    "X_train, y_train, x_dates, y_dates = train_set.X_seq, train_set.y_seq, train_set.x_dates, train_set.y_dates\n",
    "X_test, y_test, x_dates_test, y_dates_test = test_set.X_seq, test_set.y_seq, test_set.x_dates, test_set.y_dates\n",
    "\n",
    "\n",
    "cnn_model = keras.models.Sequential([\n",
    "keras.layers.Input(shape=(window_size, len(feature_cols))),\n",
    "keras.layers.Conv1D(filters=16, kernel_size=2, padding=\"valid\"),\n",
    "keras.layers.LSTM(32, return_sequences=True),\n",
    "keras.layers.LSTM(32, return_sequences=False),\n",
    "keras.layers.Dense(h)\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.004717880792838919)\n",
    "cnn_model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "history_rnn = cnn_model.fit(X_train, y_train, epochs=20,\n",
    "                validation_data=(X_test, y_test), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = cnn_model.predict(X_test)\n",
    "y_pred = y_scaler1.inverse_transform(y_pred)\n",
    "y_true = y_scaler1.inverse_transform(y_test)\n",
    "\n",
    "mse_rnn, mae_rnn, huber_rnn = compute_metrics(y_true, y_pred)\n",
    "\n",
    "cnn_lstm_record = {\n",
    "'model': 'CNN-LSTM',\n",
    "'h-step Forecast': h,\n",
    "'mse': mse_rnn,\n",
    "'mae': mae_rnn,\n",
    "'huber': huber_rnn\n",
    "}\n",
    "append_score(scores, cnn_lstm_record)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
